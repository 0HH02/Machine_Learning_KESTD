Cosas a tener en cuenta a la hora de explorar los datos:
- Estudiar si los datos son normales
- Graficar los datos


funciones de p칠rdidas a considerar:
MSE
Log(cosh(error))

funcion de activaci칩n a considerar:
https://www.datacamp.com/es/tutorial/introduction-to-activation-functions-in-neural-networks

descenso de gradiente a considerar:
BGD (batch)
SGD (stochastic)
MBGD (mini-batch)
Adam, RMSprop, Adagrad.

metricas de evaluaci칩n a considerar:
Investigar Accuracy, precision, recall, F1-score.

metricas finales a considerar:
MAE, MSE, R.

- Sobre errores numericos en pythorch:
. PyTorch Maneja Autom치ticamente:
Funciones de activaci칩n num칠ricamente estables.
Inicializaci칩n predeterminada razonable.
Herramientas para clipping de gradientes y optimizaci칩n.
Implementaci칩n eficiente de Batch Normalization (si la agregas).

Lo que T칰 Debes Hacer:
. Preprocesar los datos (normalizaci칩n o estandarizaci칩n).
Personalizar la inicializaci칩n de pesos si usas funciones susceptibles como sigmoide/tanh.
A침adir capas de Batch Normalization si es necesario.
Configurar clipping de gradiente si observas explosiones.

- Tipos de inicializaci칩n:
La inicializaci칩n de pesos es crucial para el entrenamiento eficiente de redes neuronales. Inicializar incorrectamente los pesos puede causar problemas como explosi칩n o desvanecimiento del gradiente.
Xavier (sigmoide y tanh)
He (relu)
Uniforme (todas)

-Tipos de Batch Normalization:
. BatchNorm para capas densas (tabulares)
. BatchNorm para datos de im치genes (convolucionales):
Ventajas:
Reduce el desvanecimiento/explosi칩n del gradiente.
Permite usar tasas de aprendizaje m치s altas.
Reduce la sensibilidad a la inicializaci칩n de pesos.

- Clipping:
El clipping de gradiente es una t칠cnica para manejar la explosi칩n del gradiente al limitar su magnitud m치xima. Esto es crucial en redes profundas o RNNs, donde los gradientes pueden crecer descontroladamente.
Cu치ndo Usar Clipping:
En problemas con redes profundas (como RNNs o transformers).
Si notas inestabilidad en el entrenamiento debido a gradientes extremadamente grandes.


M칠todo	Actualizaci칩n de Pesos	Ventajas	Desventajas	Uso Recomendado
Batch GD (BGD)	Usa todo el conjunto de datos para calcular un gradiente 칰nico.	- Actualizaciones estables.
- Buena convergencia en problemas convexos.	- Computacionalmente costoso para grandes datasets.
- Dif칤cil de usar con datos en streaming.	- Conjuntos de datos peque침os o que caben en memoria.
Mini-Batch GD (MBGD)	Divide los datos en lotes peque침os; actualiza par치metros para cada mini-lote.	- Balance entre estabilidad y velocidad.
- Aprovecha la paralelizaci칩n.
- Convergencia m치s r치pida.	- Menos estable que BGD.
- Requiere elegir tama침o del lote.	- Escenarios pr치cticos en general.
- Problemas con datos grandes o redes profundas.
Stochastic GD (SGD)	Calcula el gradiente usando una sola muestra por iteraci칩n.	- R치pido para datasets grandes.
- Puede escapar de m칤nimos locales.	- Gradientes ruidosos.
- Convergencia m치s lenta y menos estable.	- Problemas con grandes cantidades de datos en tiempo real.
Adam	Combina Momentum y Adagrad; ajusta din치micamente la tasa de aprendizaje.	- R치pido y eficiente.
- Adapta tasas de aprendizaje individuales.
- Robusto con datos ruidosos.	- Requiere ajuste cuidadoso de hiperpar치metros.
- Puede converger a m칤nimos sub칩ptimos.	- Redes profundas.
- Datos ruidosos.
- Escenarios gen칠ricos con arquitectura compleja.
RMSprop	Promedia los cuadrados recientes del gradiente para escalar la tasa de aprendizaje.	- Maneja gradientes din치micos.
- Funciona bien en problemas no estacionarios.
- Eficiente.	- Sensible al ajuste de la tasa de aprendizaje.
- Menos vers치til que Adam.	- Redes recurrentes (RNNs).
- Problemas con gradientes variables.
Adagrad	Ajusta la tasa de aprendizaje con base en la frecuencia de actualizaci칩n de cada par치metro.	- Excelente para datos dispersos.
- Adapta autom치ticamente la tasa de aprendizaje.	- La acumulaci칩n de gradientes puede detener el aprendizaje en iteraciones posteriores.	- Datos dispersos como en NLP o sistemas de recomendaci칩n.
- Cuando no se desea ajustar 
洧랙
풩.
